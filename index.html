<!DOCTYPE html>
<html>
<head>

  <meta charset="utf-8">
  <meta name="description"
        content="GeneFace++: Generalized and Stable Real-Time 3D Talking Face Generation">
  <meta name="keywords" content="Real3D-Portrait, One-shot, NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>GeneFace++: Generalized and Stable Real-Time 3D Talking Face Generation </title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <link rel="icon" href="./static/images/3dface_logo.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <script src="./static/js/style.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          
          <h1 class="title is-1 publication-title">
            <!-- <img src="./static/images/3dface_logo.png"class="interpolation-image" width="4%"/> -->
            GeneFace++: Generalized and Stable <p></p>
            Real-Time 3D Talking Face Generation </h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              Anonymous Authors</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- Code Link. -->
              <span class="link-block">
                <a href="" class="external-link button is-normal is-rounded is-dark" onclick="alert('We plan to release the source code after the rebuttal phase.')">
                <span class="icon">
                <svg class="svg-inline--fa fa-github fa-w-16" aria-hidden="true" focusable="false" data-prefix="fab" data-icon="github" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512" data-fa-i2svg=""><path fill="currentColor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg><!-- <i class="fab fa-github"></i> Font Awesome fontawesome.com -->
                </span>
                <span>Code</span>
                </a>
                </span>
            </div>
          </div>

            <!-- Abstract. -->
          <div class="container is-max-desktop">
            <div class="columns is-centered has-text-centered">
              <div class="column is-four-fifths">
                <h2 class="title is-3">Abstract</h2>
                <div class="content has-text-justified">
                  <p>
                    Generating talking person portraits given arbitrary speech input is a crucial
                     problem in the field of digital human. A modern talking face generation
                     system is expected to achieve the goals of generalized audio-lip synchronization, 
                     good video quality, and high system efficiency. Recently, neural radiance field (NeRF)
                      has become a popular rendering technique in this field since it could achieve 
                      high-fidelity and 3D-consistent talking face generation with a few-minute-long training 
                      video. However, there still exist several challenges for NeRF-based methods: 
                      1) as for the <b>lip synchronization,</b> it is hard to generate a long facial motion sequence
                       of high lip accuracy and temporal consistency; 2) as for the <b>video quality</b>,
                        due to the limited data used to train the renderer, it is vulnerable to out-of-domain
                         motion condition and produce bad rendering results occasionally; 3) as for the <b>system 
                          efficiency</b>, the slow training and inference speed of the vanilla NeRF severely obstruct
                          its usage in real-world applications. 
                        </p>
                        <p>                     
                      In this paper, we propose <u><b>GeneFace++</b></u> to handle these challenges by: 1)
                       utilizing the <u><b>pitch contour</b></u> as an auxiliary feature and introducing a temporal 
                       loss in the facial motion prediction process; 2) proposing a <u><b>landmark locally 
                        linear embedding</b></u> method to regulate the outliers in the predicted motion sequence
                        to avoid robustness issues; 3) designing a <u><b>instant motion-to-video renderer</b></u> to 
                        achieves fast training and real-time inference. With these settings, GeneFace++
                         becomes the first NeRF-based method that achieves stable and <b>nearly 2-times real-time (45 FPS on RTX3090Ti) </b>
                         talking face generation with generalized audio-lip synchronization. Extensive experiments 
                          show that our method outperforms state-of-the-art baselines in terms of subjective 
                          and objective evaluation.

                  </p>
                </div>
              </div>
            </div>
            </div>
    <!--/ Abstract. -->

        </div>
      </div>
      <h2 class="title is-3">Overall Pipeline</h2>
      The inference process of <b>GeneFace++</b> is shown as follows:
      <img src="./static/images/Inference.png"
      class="interpolation-image"
      alt="The inference process of Real3D-Portrait."/>
    </div>
  </div>
<!-- </section> -->

<!-- <section class="section"> -->
  
  <div class="container is-max-desktop">

            <!-- Demo Main -->
            <h1 class="title is-3">Demo 1: GeneFace++ of 2 identites driven by 6 languages</h1>
            <div class="content has-text-justified">
              <p>
                We provide a demo video in which our <b><u>GeneFace++</u></b> of 2 identities (May and Obama) are driven by audio clips from 6 languages (English, Chinese, French, German, Korean, and Japanese), to show that we achieve the three goals of modern talking face system: 
                <ol>
                  <li><b>generalized lip synchornization</b> (We generate lip-sync results to 6 languages); </li>
                  <li><b>good video quality</b> (We generate high-qualty videos with rich identity-specific details and 3D consistency); </li>
                  <li><b>high system efficency</b> (The 512x512-resolution video frames are generated by our instant motion-to-video model in 50+ FPS on a Nvidia A100). </li>
                </ol>
              </p>
            </div>
            <div class="content has-text-centered">
              <video id="replay-video"
                    controls
                    preload
                    playsinline
                    width="90%">
                <source src="./static/videos/May_Obama_demo.mp4"
                        type="video/mp4">
              </video>
            </div>
            <!--/ Demo Main -->

          <!-- Demo Main -->
          <h1 class="title is-3">Demo 2: All methods driven by a 3-minute-long song</h1>
          <div class="content has-text-justified">
            <p>
              To further show the good lip-sync generalizability of <b><u>GeneFace++</u></b>, in the following video, we provide a hard case, in which all methods are driven by a <u><b>three-minute-long song</u></b>.
            </p>
          </div>
          <div class="content has-text-centered">
            <video id="replay-video"
                  controls
                  preload
                  playsinline
                  width="90%">
              <source src="./GeneFace++/dream_it_possible.mp4"
                      type="video/mp4">
            </video>
          </div>
          <!--/ Demo Main -->



          <h2 class="title is-3">Demo 3: Ablations on the Instant Motion-to-Video renderer</h2>
          We visualize the following ablations: 1) whether non-face regularization loss in the training NeRF stage could address the temporal jittering problem in the non-face area; 2) whether the SR module improves the image fidelity of the NeRF renderer while keeping the 3D consistency.

          <div class="columns is-centered">
            <!-- Large Pose -->
            <div class="column">

              <div class="content">
                <p></p>
                <h3 class="title is-4">1. w./w.o. non-face reg loss</h3>
                <p>
                  We can see that with the non-face reg loss, the rendered head is more temporal stable and realistic.
                </p>
                <video id="dollyzoom" controls playsinline height="100%">
                  <source src="./static/videos/w_wo_nonface_reg.mp4"
                          type="video/mp4">
                </video>
              </div>
            </div>
            <!--/ Large Pose -->
      
            <!-- BG. -->
            <div class="column">

              <div class="columns is-centered">
                <div class="column content">
                <p></p>
                <h3 class="title is-4">2. w./w.o. SR module</h3>
                  
                  <p><b>Click the image below</b> to open a html that could control a slider to compare two images:</p>
                    <!-- <img src="static/images/w_wo_SR.png" onclick="location.href='./static/pages/compare_with_without_sr.html'"> -->
                    <img src="static/images/w_wo_SR.png" onclick="window.open('./static/pages/compare_with_without_sr.html')" style="cursor: pointer">

                </div>
      
              </div>
            </div>
          </div>

          <h2 class="title is-3">Demo 4: Text-driven talking face generation</h2>
          In the following video, we show the potential of our GeneFace++ to achieve <b>text-driven talking face generation</b>. We first use a zero-shot TTS model with the Obama's voice as prompts to generate the audio track given arbitrary text inputs, then use the synthesized audio track to drive our GeneFace++ to obtain the video track. Both of the left and right sub-videos are synthesized by GeneFace++, yet given different head pose sequence. 
        


          <!-- Welcome video -->
        <section class="hero is-light is-small">
          <div class="hero-body">
            <div class="container">
                <div class="item item-steve">
                  <video poster="" id="steve" controls playsinline height="100%">
                    <source src="./static/videos/GeneFace++withTTS.mp4"
                            type="video/mp4">
                  </video>
                </div>
            </div>
          </div>
        </section>
      </div>
          </div>

          <!--/ BG. -->
      
          




    <!-- Welcome video -->

</body>
</html>
